{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " ### IMPLEMENTATION OF BACKPROPAGATION"
      ],
      "metadata": {
        "id": "0JORlUOnplRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Backpropagation** is a fundamental algorithm used for training neural networks. It is the process by which a neural network **learns** by adjusting its weights and biases based on the error or difference between the predicted output and the actual target. This error is propagated backward through the network to update the weights and minimize the loss function, thereby improving the model's accuracy over time.\n",
        "\n",
        "* the **sigmoid function** is a type of activation function that maps input values to a range between 0 and 1\n",
        "* Using **np.random.seed(**) in your code is crucial for controlling the randomness in your computations, particularly when working with random number generation in NumPy.\n",
        "*  **return x * (1 - x)** : This derivative is used during backpropagation to calculate how much the weights need to be adjusted.  \n",
        "* **Hidden Layer Activation**: Calculates the activation of the hidden layer by multiplying the inputs with the weights between the input and hidden layers."
      ],
      "metadata": {
        "id": "drYCKodcp8WP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOx0d1b3v-nb",
        "outputId": "1aba8726-068f-4db8-d41d-30e34ee8837a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1000, Loss: 0.39557039180027914\n",
            "Epoch 2000, Loss: 0.268978650120054\n",
            "Epoch 3000, Loss: 0.20877501367079165\n",
            "Epoch 4000, Loss: 0.17388635703099511\n",
            "Epoch 5000, Loss: 0.15095324126862225\n",
            "Epoch 6000, Loss: 0.13460178571028433\n",
            "Epoch 7000, Loss: 0.12227213430243797\n",
            "Epoch 8000, Loss: 0.11258998461667526\n",
            "Epoch 9000, Loss: 0.10475002443046719\n",
            "Epoch 10000, Loss: 0.09824792584007563\n",
            "Final weights between input and hidden layers: \n",
            " [[0.90571962 7.32475086]\n",
            " [0.90572643 7.32731531]]\n",
            "Final weights between hidden and output layers: \n",
            " [[-27.46354604]\n",
            " [ 21.74964594]]\n",
            "Predicted outputs after training: \n",
            " [[0.05432736]\n",
            " [0.89823995]\n",
            " [0.89824011]\n",
            " [0.13514441]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation function: Sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of the sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training data\n",
        "inputs = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [1, 0],\n",
        "                   [1, 1]])\n",
        "\n",
        "outputs = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Initialize weights randomly with mean 0\n",
        "input_layer_neurons = 2\n",
        "hidden_layer_neurons = 2\n",
        "output_neurons = 1\n",
        "\n",
        "weights_input_hidden = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
        "weights_hidden_output = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.5\n",
        "\n",
        "# Number of iterations\n",
        "epochs = 10000\n",
        "\n",
        "# Training process\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation\n",
        "    hidden_layer_activation = np.dot(inputs, weights_input_hidden)\n",
        "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
        "\n",
        "    final_layer_activation = np.dot(hidden_layer_output, weights_hidden_output)\n",
        "    predicted_output = sigmoid(final_layer_activation)\n",
        "\n",
        "    # Compute the error\n",
        "    error = outputs - predicted_output\n",
        "\n",
        "    # Backpropagation\n",
        "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
        "\n",
        "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
        "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    # Updating the weights\n",
        "    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
        "    weights_input_hidden += inputs.T.dot(d_hidden_layer) * learning_rate\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        loss = np.mean(np.abs(error))\n",
        "        print(f'Epoch {epoch + 1}, Loss: {loss}')\n",
        "\n",
        "# Testing the trained neural network\n",
        "print(\"Final weights between input and hidden layers: \\n\", weights_input_hidden)\n",
        "print(\"Final weights between hidden and output layers: \\n\", weights_hidden_output)\n",
        "print(\"Predicted outputs after training: \\n\", predicted_output)\n"
      ]
    }
  ]
}